# Hyperparameters based on:
#   "Soft Actor-Critic Algorithms and Applications"
#   by Tuomas Haarnoja et al.
#   https://arxiv.org/abs/1812.05905
# The learning rate was taken from:
#   "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images"
#   by Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, Rob Fergus
#   https://arxiv.org/abs/1910.01741
import dopamine.continuous_domains.run_experiment
import dopamine.discrete_domains.gym_lib
import dopamine.labs.sac_from_pixels.deepmind_control_lib
import dopamine.labs.sac_from_pixels.continuous_networks
import dopamine.jax.agents.dqn.dqn_agent
import dopamine.jax.agents.sac.sac_agent
import dopamine.tf.replay_memory.circular_replay_buffer

SACAgent.reward_scale_factor = 1
SACAgent.network = @continuous_networks.SACConvNetwork
SACAgent.num_layers = 4  # num hidden layers = 2
SACAgent.hidden_units = 1024
SACAgent.gamma = 0.99
SACAgent.update_horizon = 1
SACAgent.min_replay_history = 1000  # agent steps
SACAgent.update_period = 2
SACAgent.target_update_type = 'soft'
SACAgent.target_smoothing_coefficient = 0.01
SACAgent.target_entropy = None  # Defaults to -num_action_dims/2
SACAgent.optimizer = 'adam'
SACAgent.seed = None  # Seed with the current time
SACAgent.stack_size = 3
SACAgent.observation_dtype = %sac_agent.IMAGE_DTYPE
create_optimizer.learning_rate = 1e-3
create_optimizer.beta1 = 0.9
create_optimizer.beta2 = 0.9
create_optimizer.eps = 1.0e-8

create_continuous_runner.schedule = 'continuous_train'
deepmind_control_lib.create_deepmind_control_environment.use_image_observations = True
deepmind_control_lib.create_deepmind_control_environment.domain_name = 'cheetah'
deepmind_control_lib.create_deepmind_control_environment.task_name = 'run'
DeepmindControlPreprocessing.action_repeat = 4
ContinuousTrainRunner.create_environment_fn = @deepmind_control_lib.create_deepmind_control_environment
ContinuousRunner.num_iterations = 100
ContinuousRunner.training_steps = 1000
ContinuousRunner.evaluation_steps = 1000  # agent steps
ContinuousRunner.max_steps_per_episode = 250
ContinuousRunner.clip_rewards = False

ReplayBuffer.batch_size = 128
ReplayBuffer.max_capacity = 1000000
ReplayBuffer.checkpoint_duration = 1

encoder_feature_dim = 50
encoder_tau = 0.05
encoder_lr = 1e-3
decoder_lr = 1e-3
decoder_update_freq = 1
decoder_latent_lambda = 1e-6
decoder_weight_lambda = 1e-7
